# -*- coding: utf-8 -*-
"""Prod Rec

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vePLjaY945m2nqkdyDbTRwLXMQKZKxUS

# Generate Data
"""

import pandas as pd
import numpy as np

# Define number of samples
num_samples = 1000  # You can increase this for more data

# Generate random product data
product_ids = np.arange(1, num_samples + 1)
names = [f"Product {i}" for i in product_ids]
descriptions = [f"This is a description for Product {i}" for i in product_ids]

# Define consistent categories and subcategories
categories_subcategories = {
    "Clothing": ["T-Shirts", "Jeans", "Jackets", "Dresses", "Shorts"],
    "Footwear": ["Shoes", "Sneakers", "Sandals", "Boots", "Slippers"],
    "Electronics": ["Mobiles", "Laptops", "Headphones", "Cameras", "Smartwatches"],
    "Home": ["Furniture", "Lighting", "Decor", "Kitchenware", "Bedding"],
    "Sports": ["Gym", "Outdoor", "Fitness", "Cycling", "Yoga"]
}

# Generate consistent categories and subcategories
categories = np.random.choice(list(categories_subcategories.keys()), num_samples)
subcategories = [np.random.choice(categories_subcategories[cat]) for cat in categories]

# Generate related tags based on categories and subcategories
tags = []
for cat, subcat in zip(categories, subcategories):
    if cat == "Clothing":
        tags.append(",".join(np.random.choice(["casual", "summer", "trendy", "sale", "new", "cotton", "fashion"], size=3)))
    elif cat == "Footwear":
        tags.append(",".join(np.random.choice(["comfort", "running", "sport", "leather", "new", "waterproof"], size=3)))
    elif cat == "Electronics":
        tags.append(",".join(np.random.choice(["smart", "tech", "gadget", "new", "sale", "wireless"], size=3)))
    elif cat == "Home":
        tags.append(",".join(np.random.choice(["modern", "decor", "sale", "new", "eco-friendly", "wooden"], size=3)))
    elif cat == "Sports":
        tags.append(",".join(np.random.choice(["fitness", "outdoor", "training", "new", "sale", "durable"], size=3)))

# Generate base prices based on categories
base_prices = []
for cat in categories:
    if cat == "Clothing":
        base_prices.append(np.round(np.random.uniform(10, 100), 2))
    elif cat == "Footwear":
        base_prices.append(np.round(np.random.uniform(50, 200), 2))
    elif cat == "Electronics":
        base_prices.append(np.round(np.random.uniform(100, 1000), 2))
    elif cat == "Home":
        base_prices.append(np.round(np.random.uniform(20, 500), 2))
    elif cat == "Sports":
        base_prices.append(np.round(np.random.uniform(30, 300), 2))

base_prices = np.array(base_prices)

# Generate brands based on categories
brands = []
for cat in categories:
    if cat == "Clothing":
        brands.append(np.random.choice(["Nike", "Adidas", "Zara", "H&M", "Uniqlo"]))
    elif cat == "Footwear":
        brands.append(np.random.choice(["Nike", "Adidas", "Puma", "Reebok", "Skechers"]))
    elif cat == "Electronics":
        brands.append(np.random.choice(["Samsung", "Apple", "Sony", "LG", "Xiaomi"]))
    elif cat == "Home":
        brands.append(np.random.choice(["IKEA", "Home Depot", "Williams-Sonoma", "Crate & Barrel", "Wayfair"]))
    elif cat == "Sports":
        brands.append(np.random.choice(["Nike", "Adidas", "Under Armour", "Puma", "Reebok"]))

# Generate user behavior data
ratings = np.random.randint(0, 500, num_samples)  # Number of ratings received
average_ratings = np.round(np.random.uniform(1, 5, num_samples), 1)  # Average rating score
sold_quantities = np.random.randint(0, 1000, num_samples)  # Total units sold
view_counts = np.random.randint(0, 10000, num_samples)  # Number of times viewed
order_counts = np.random.randint(0, 5000, num_samples)  # Number of times ordered
wishlist_counts = np.random.randint(0, 1000, num_samples)  # Wishlist count
return_rates = np.round(np.random.uniform(0, 20, num_samples), 1)  # Return percentage
discounted_prices = np.round(base_prices * np.random.uniform(0.5, 1.0, num_samples), 2)  # Discounted price

# Create DataFrame
dataset = pd.DataFrame({
    "product_id": product_ids,
    "name": names,
    "description": descriptions,
    "category_name": categories,
    "subcategory_name": subcategories,
    "tags": tags,
    "basePrice": base_prices,
    "brand": brands,
    "ratings": ratings,
    "average_rating": average_ratings,
    "sold_quantity": sold_quantities,
    "view_count": view_counts,
    "order_count": order_counts,
    "wishlist_count": wishlist_counts,
    "return_rate": return_rates,
    "discounted_price": discounted_prices
})

# Save the dataset
dataset.to_csv("random_product_data.csv", index=False)

# Display first few rows
print(dataset.head())

import pandas as pd
import numpy as np

# Define number of samples
num_samples = 1000  # Increase for more data

# Define structured categories, subcategories, and brands
category_mapping = {
    "Clothing": {"subcategories": ["T-Shirts", "Jackets", "Jeans", "Dresses"], "brands": ["Nike", "Adidas", "Puma"]},
    "Footwear": {"subcategories": ["Sneakers", "Sandals", "Boots", "Formal Shoes"], "brands": ["Nike", "Adidas", "Reebok"]},
    "Electronics": {"subcategories": ["Mobiles", "Laptops", "Headphones", "Smartwatches"], "brands": ["Samsung", "Sony", "Apple"]},
    "Home": {"subcategories": ["Furniture", "Decor", "Lighting", "Kitchen"], "brands": ["IKEA", "Philips", "HomeTown"]},
    "Sports": {"subcategories": ["Gym Equipment", "Bicycles", "Outdoor", "Sportswear"], "brands": ["Nike", "Decathlon", "Adidas"]}
}

# Generate product IDs
product_ids = np.arange(1, num_samples + 1)
names = [f"Product {i}" for i in product_ids]
descriptions = [f"This is a description for Product {i}" for i in product_ids]

# Generate aligned category, subcategory, and brand
categories = np.random.choice(list(category_mapping.keys()), num_samples)
subcategories = [np.random.choice(category_mapping[cat]["subcategories"]) for cat in categories]
brands = [np.random.choice(category_mapping[cat]["brands"]) for cat in categories]

# Generate relevant tags per category
tag_pool = {
    "Clothing": ["fashion", "casual", "summer", "trendy"],
    "Footwear": ["sports", "running", "comfortable", "sale"],
    "Electronics": ["tech", "new", "smart", "gadgets"],
    "Home": ["decor", "modern", "handmade", "luxury"],
    "Sports": ["outdoor", "fitness", "athletic", "performance"]
}
tags = [",".join(np.random.choice(tag_pool[cat], size=2, replace=False)) for cat in categories]

# Generate price and discounts
base_prices = np.round(np.random.uniform(10, 1000, num_samples), 2)
discount_rates = np.random.uniform(0.1, 0.5, num_samples)
discounted_prices = np.round(base_prices * (1 - discount_rates), 2)

# Generate user behavior data
ratings = np.random.randint(1, 1000, num_samples)  # Number of ratings received
average_ratings = np.round(np.random.uniform(1, 5, num_samples), 1)  # Avg rating score
sold_quantities = np.random.randint(0, 2000, num_samples)  # Total units sold
view_counts = np.random.randint(0, 50000, num_samples)  # Number of times viewed
order_counts = np.random.randint(0, 10000, num_samples)  # Number of times ordered
wishlist_counts = np.random.randint(0, 5000, num_samples)  # Wishlist count
return_rates = np.round(np.random.uniform(0, 20, num_samples), 1)  # Return percentage

# Create DataFrame
dataset = pd.DataFrame({
    "product_id": product_ids,
    "name": names,
    "description": descriptions,
    "category_name": categories,
    "subcategory_name": subcategories,
    "tags": tags,
    "base_price": base_prices,
    "discounted_price": discounted_prices,
    "brand": brands,
    "ratings": ratings,
    "average_rating": average_ratings,
    "sold_quantity": sold_quantities,
    "view_count": view_counts,
    "order_count": order_counts,
    "wishlist_count": wishlist_counts,
    "return_rate": return_rates
})

# Save the dataset
dataset.to_csv("enhanced_product_data.csv", index=False)

# Display first few rows
display(dataset.head())  # Display the first few rows of the dataframe
print("Enhanced Product Dataset:")

display(dataset.head())  # Display the first few rows of the dataframe
print("Enhanced Product Dataset:")

!pip install faker==18.11.1

import pandas as pd
import numpy as np
from faker import Faker

# Initialize Faker for generating fake data
fake = Faker()

# Parameters
num_users = 1000  # Number of users
num_products = 500  # Number of products
num_interactions = 10000  # Number of user-product interactions

# Generate synthetic users
users = [fake.unique.random_int(min=1, max=num_users) for _ in range(num_users)]

# Generate synthetic products
products = [fake.unique.random_int(min=1, max=num_products) for _ in range(num_products)]

# Generate synthetic user-product interactions
data = {
    "user_id": np.random.choice(users, num_interactions),
    "product_id": np.random.choice(products, num_interactions),
    "rating": np.random.randint(1, 6, num_interactions),  # Ratings between 1 and 5
    "quantity": np.random.randint(1, 11, num_interactions),  # Quantity purchased (1-10)
    "timestamp": pd.date_range(start="2020-01-01", periods=num_interactions, freq="T"),  # Timestamps
}

# Create DataFrame
df = pd.DataFrame(data)

# Add additional product metadata (optional)
product_categories = np.random.choice(["Electronics", "Clothing", "Home", "Books", "Toys"], num_products)
product_prices = np.round(np.random.uniform(10, 500, num_products), 2)
product_metadata = {
    "product_id": products,
    "category": product_categories,
    "price": product_prices,
}

# Create product metadata DataFrame
df_products = pd.DataFrame(product_metadata)

# Save datasets to CSV
df.to_csv("user_product_interactions.csv", index=False)
df_products.to_csv("product_metadata.csv", index=False)

print("Dataset generated and saved to CSV files!")
print(df.head())
print(df_products.head())

"""# Content Based"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Load the dataset
dataset = pd.read_csv("random_product_data.csv")

# Combine relevant features into a single text column for TF-IDF
dataset["combined_features"] = (
    dataset["category_name"] + " " +
    dataset["subcategory_name"] + " " +
    dataset["tags"] + " " +
    dataset["brand"]
)

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words="english")
tfidf_matrix = tfidf.fit_transform(dataset["combined_features"])

# Compute cosine similarity matrix
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Function to get product recommendations
def get_recommendations(product_id, top_n=5):
    """
    Get top N product recommendations based on cosine similarity.

    Args:
        product_id (int): The ID of the product to find recommendations for.
        top_n (int): Number of recommendations to return.

    Returns:
        DataFrame: Recommended products.
    """
    # Find the index of the product
    product_index = dataset[dataset["product_id"] == product_id].index[0]

    # Get similarity scores for the product
    sim_scores = list(enumerate(cosine_sim[product_index]))

    # Sort products by similarity score
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Get top N similar products (excluding the product itself)
    sim_scores = sim_scores[1:top_n + 1]

    # Get product indices
    product_indices = [i[0] for i in sim_scores]

    # Return recommended products
    return dataset.iloc[product_indices]

# Example: Get recommendations for product_id = 1
recommendations = get_recommendations(product_id=1, top_n=20)
# print(recommendations[["product_id", "name", "category_name", "subcategory_name", "brand", "average_rating"]])
# sort by average rating
recommendations = recommendations.sort_values(by="average_rating", ascending=False)
print(recommendations[["product_id", "name", "category_name", "subcategory_name", "brand", "average_rating"]])

dataset.head()

"""# Collaborative filtering"""

pip install scikit-surprise

import pandas as pd
import numpy as np

# Simulate user-item interaction data (user_id, product_id, rating)
num_users = 100
num_products = 1000
num_interactions = 5000

user_ids = np.random.randint(1, num_users + 1, num_interactions)
product_ids = np.random.randint(1, num_products + 1, num_interactions)
ratings = np.random.randint(1, 6, num_interactions)  # Ratings from 1 to 5

interaction_data = pd.DataFrame({
    "user_id": user_ids,
    "product_id": product_ids,
    "rating": ratings
})

# Save the interaction data
interaction_data.to_csv("user_product_interactions.csv", index=False)

interaction_data.head()

from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split
from surprise.accuracy import rmse

# Load interaction data into Surprise
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(interaction_data[["user_id", "product_id", "rating"]], reader)

# Split data into train and test sets
trainset, testset = train_test_split(data, test_size=0.2)

# Train an SVD model (Matrix Factorization)
model = SVD()
model.fit(trainset)

# Evaluate the model
predictions = model.test(testset)
print("RMSE:", rmse(predictions))

def get_collaborative_filtering_recommendations(user_id, top_n=5):
    """
    Get top N product recommendations for a user using collaborative filtering.

    Args:
        user_id (int): The ID of the user to recommend products for.
        top_n (int): Number of recommendations to return.

    Returns:
        list: Recommended product IDs.
    """
    # Get a list of all product IDs
    all_product_ids = interaction_data["product_id"].unique()

    # Predict ratings for all products
    predictions = []
    for product_id in all_product_ids:
        predicted_rating = model.predict(user_id, product_id).est
        predictions.append((product_id, predicted_rating))

    # Sort products by predicted rating
    predictions.sort(key=lambda x: x[1], reverse=True)

    # Get top N product IDs
    top_product_ids = [x[0] for x in predictions[:top_n]]

    return top_product_ids

# Example: Get recommendations for user_id = 1
recommended_products = get_collaborative_filtering_recommendations(user_id=1, top_n=5)
print("Recommended Product IDs:", recommended_products)



"""# Hybrid"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Load product data
product_data = pd.read_csv("random_product_data.csv")

# Combine relevant features into a single text column for TF-IDF
product_data["combined_features"] = (
    product_data["category_name"] + " " +
    product_data["subcategory_name"] + " " +
    product_data["tags"] + " " +
    product_data["brand"]
)

# Initialize TF-IDF Vectorizer
tfidf = TfidfVectorizer(stop_words="english")
tfidf_matrix = tfidf.fit_transform(product_data["combined_features"])

# Compute cosine similarity matrix
content_similarity = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Function to get content-based recommendations
def get_content_based_recommendations(product_id, top_n=5):
    """
    Get top N product recommendations based on content similarity.
    """
    product_index = product_data[product_data["product_id"] == product_id].index[0]
    sim_scores = list(enumerate(content_similarity[product_index]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n + 1]
    product_indices = [i[0] for i in sim_scores]
    return product_data.iloc[product_indices]

from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split

# Load interaction data
interaction_data = pd.read_csv("user_product_interactions.csv")

# Load data into Surprise
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(interaction_data[["user_id", "product_id", "rating"]], reader)

# Split data into train and test sets
trainset, testset = train_test_split(data, test_size=0.2)

# Train an SVD model
collab_model = SVD()
collab_model.fit(trainset)

# Function to get collaborative filtering recommendations
def get_collaborative_filtering_recommendations(user_id, top_n=5):
    """
    Get top N product recommendations for a user using collaborative filtering.
    """
    all_product_ids = product_data["product_id"].unique()
    predictions = []
    for product_id in all_product_ids:
        predicted_rating = collab_model.predict(user_id, product_id).est
        predictions.append((product_id, predicted_rating))
    predictions.sort(key=lambda x: x[1], reverse=True)
    top_product_ids = [x[0] for x in predictions[:top_n]]
    return product_data[product_data["product_id"].isin(top_product_ids)]

def get_hybrid_recommendations(user_id, product_id, top_n=5, content_weight=0.5, collab_weight=0.5):
    """
    Get hybrid recommendations by combining content-based and collaborative filtering.

    Args:
        user_id (int): The ID of the user for collaborative filtering.
        product_id (int): The ID of the product for content-based filtering.
        top_n (int): Number of recommendations to return.
        content_weight (float): Weight for content-based recommendations.
        collab_weight (float): Weight for collaborative filtering recommendations.

    Returns:
        DataFrame: Hybrid recommendations.
    """
    # Get content-based recommendations
    content_recs = get_content_based_recommendations(product_id, top_n)
    content_recs["score"] = content_weight  # Assign a weight to content-based recommendations

    # Get collaborative filtering recommendations
    collab_recs = get_collaborative_filtering_recommendations(user_id, top_n)
    collab_recs["score"] = collab_weight  # Assign a weight to collaborative filtering recommendations

    # Combine recommendations
    hybrid_recs = pd.concat([content_recs, collab_recs])

    # Group by product_id and aggregate scores
    hybrid_recs = hybrid_recs.groupby("product_id").agg({"score": "sum"}).reset_index()

    # Sort by combined score
    hybrid_recs = hybrid_recs.sort_values(by="score", ascending=False).head(top_n)

    # Merge with product data to get product details
    hybrid_recs = pd.merge(hybrid_recs, product_data, on="product_id")

    return hybrid_recs

# Example: Get hybrid recommendations for user_id = 1 and product_id = 1
hybrid_recommendations = get_hybrid_recommendations(user_id=1, product_id=1, top_n=5)
print(hybrid_recommendations[["product_id", "name", "category_name", "subcategory_name", "brand", "score"]])